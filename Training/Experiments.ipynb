{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as t\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "from MobileNetV2 import MobileNetV2\n",
    "\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics  import f1_score\n",
    "\n",
    "import io\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AmazonFashionDatasetLoader(data_utils.Dataset):\n",
    "    def __init__(self, datapath, transforms, train=True):\n",
    "        [user_train, user_validation, user_test, Item, self.usernum, self.itemnum] = self.load_numpy(datapath)\n",
    "        self.categories_dict, self.num_categories = self.count_categories(Item)\n",
    "        self.item_dict = self.build_item_dict(Item)\n",
    "        self.transforms = transforms\n",
    "        random.seed(SEED)\n",
    "        self.dataset, self.max_length = self.preprocess(user_train) if train else self.preprocess(user_validation)\n",
    "        \n",
    "    def load_numpy(self, datapath, encoding='bytes'):\n",
    "        return np.load(datapath, encoding=encoding)\n",
    "    \n",
    "    def process_categories(self, category_list):\n",
    "        categories = list(itertools.chain.from_iterable(category_list))\n",
    "        categories = [x.decode('utf8').split(',') for x in categories]\n",
    "        categories = list(set(itertools.chain.from_iterable(categories)))\n",
    "        return categories\n",
    "    \n",
    "    def count_categories(self, items):\n",
    "        category_set = set()\n",
    "        for i in range(self.itemnum):\n",
    "            item = items[i]\n",
    "            for category in self.process_categories(item[b'categories']):\n",
    "                category_set.add(category)\n",
    "        categories = list(category_set)\n",
    "        categories = {k: v for v, k in enumerate(categories)}\n",
    "        return categories, len(categories)\n",
    "    \n",
    "    def to_one_hot(self, categories):\n",
    "        indices = np.array(categories)\n",
    "        onehot = np.zeros(self.num_categories)\n",
    "        onehot[indices] = 1\n",
    "        return onehot\n",
    "    \n",
    "    def build_item_dict(self, items):\n",
    "        asin_to_pic = {}\n",
    "        for i in range(self.itemnum):\n",
    "            item = items[i]\n",
    "            asin = item[b'asin'].decode('utf8')\n",
    "            asin_to_pic[asin] = {}\n",
    "            asin_to_pic[asin]['img'] = item[b'imgs']\n",
    "            category_indices = [self.categories_dict[x] for x in self.process_categories(item[b'categories'])]\n",
    "            asin_to_pic[asin]['label'] = self.to_one_hot(category_indices)\n",
    "        return asin_to_pic\n",
    "    \n",
    "    def preprocess(self, numpy_dataset):\n",
    "        max_length = 0\n",
    "        dataset = []\n",
    "        for i in range(self.usernum):\n",
    "            user = numpy_dataset[i]\n",
    "            items = [entry['asin'] for entry in user]\n",
    "            if max_length < len(items):\n",
    "                max_length = len(items)\n",
    "            imgs = [self.item_dict[asin]['img'] for asin in items]\n",
    "            labels = [self.item_dict[asin]['label'] for asin in items]\n",
    "            labels = np.clip(np.sum(np.array(labels), axis=0), a_min=0, a_max=1)\n",
    "            dataset.append({'imgs': imgs, 'labels': labels})\n",
    "        \n",
    "        random.shuffle(dataset)\n",
    "        return dataset, max_length\n",
    "    \n",
    "    def preprocess_img(self, img):\n",
    "        img = Image.open(io.BytesIO(img)).convert('RGB')\n",
    "        return img            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        user = self.dataset[index]\n",
    "        \n",
    "        X = [self.preprocess_img(entry) for entry in user['imgs']]\n",
    "        X = [self.transforms(img) for img in X]\n",
    "        Y = user['labels']\n",
    "        return torch.stack(X), torch.from_numpy(np.array(Y)).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.usernum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AmazonFashionExtractedDataset(data_utils.Dataset):\n",
    "    def __init__(self, datapath_x, datapath_y, train=True):\n",
    "        self.dataset = np.load(datapath_x)\n",
    "        self.labels = np.load(datapath_y)\n",
    "        \n",
    "        self.dataset = self.dataset[:int(self.dataset.shape[0]*0.85)] if train else self.dataset[int(self.dataset.shape[0]*0.85):]\n",
    "        self.labels = self.labels[:int(self.labels.shape[0]*0.85)] if train else self.labels[int(self.labels.shape[0]*0.85):]\n",
    "        print('length of X = %d, length of Y = %d' % (len(self.dataset), len(self.labels)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.dataset[index]).float(), torch.from_numpy(self.labels[index]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    y_hat = np.squeeze(torch.sigmoid(predictions).to('cpu').numpy()) >= 0.5\n",
    "    y_hat = y_hat.astype('int')\n",
    "    \n",
    "    y = np.squeeze(labels.to('cpu').numpy())\n",
    "    \n",
    "    precision = precision_score(y, y_hat, average='weighted')\n",
    "    recall = recall_score(y, y_hat, average='weighted')\n",
    "    f1 = f1_score(y, y_hat, average='weighted')\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_and_save_features(extractor, data, length, train):\n",
    "    model.eval()\n",
    "    x_file = 'amazon_fashion_x_train' if train else 'amazon_fashion_x_test'\n",
    "    y_file = 'amazon_fashion_y_train' if train else 'amazon_fashion_y_test'\n",
    "    with torch.no_grad():\n",
    "        x_complete = []\n",
    "        y_complete = []\n",
    "        for i in range(length):\n",
    "            X, Y = data[i]\n",
    "            if X.shape[0] == 1:\n",
    "                continue\n",
    "            x_complete.append(extractor(X).numpy())\n",
    "            y_complete.append(Y.numpy())\n",
    "\n",
    "            if (i % 10 == 9):\n",
    "                print('%d / %d' % (i, length))\n",
    "            if (i % 1000 == 999):\n",
    "                np.save(x_file, x_complete)\n",
    "                np.save(y_file, y_complete)\n",
    "        \n",
    "        np.save(x_file, x_complete)\n",
    "        np.save(y_file, y_complete)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SqueezeExpandAttention(nn.Module):\n",
    "    def __init__(self, embeding_dimension, squeeze_dimension):\n",
    "        super(SqueezeExpandAttention, self)\n",
    "        self.squeeze = nn.Linear(embeding_dimension, squeeze_dimension)\n",
    "        self.q0 = nn.Linear(squeeze_dimension, 1)\n",
    "        self.W = nn.Linear(squeeze_dimension, squeeze_dimension)\n",
    "        self.expand = nn.Linear(squeeze_dimension, embeding_dimension)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = b x n x k\n",
    "        squeezed = self.squeeze(x) # b x n x s\n",
    "        \n",
    "        r0 = self.q0(squeezed) # b x n x 1\n",
    "        r0 = F.softmax(r0, dim=1)\n",
    "        r0 = torch.matmul(torch.transpose(x, 1, 2), r0) # (b x k x n) times (b x n x 1) = (b x k x 1)\n",
    "        r0 = torch.squeeze(r0, dim=-1) # b x k\n",
    "        r0 = self.squeeze(r0) # b x s\n",
    "        \n",
    "        q1 = self.W(r0) # b x s\n",
    "        q1 = torch.squeeze(q1, dim=1)\n",
    "        q1 = torch.tanh(q1)\n",
    "        q1 = self.expand(q1) # b x k\n",
    "        \n",
    "        r1 = torch.matmul(x, torch.unsqueeze(q1, dim=-1)) # (b x n x k) times (b x k x 1) = (b x n x 1)\n",
    "        r1 = r1.squeeze(dim=-1)\n",
    "        r1 = F.softmax(r1, dim=1) # b x n\n",
    "        r1 = torch.matmul(torch.transpose(x, 1, 2), torch.unsqueeze(r1, dim=-1)) # (b x k x n) times (b x n x 1) = (b x k x 1)\n",
    "        r1 = torch.squeeze(r1, dim=-1) # b x k\n",
    "        #r1 = torch.squeeze(r1, dim=1)\n",
    "        return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FCAttentionModule(nn.Module):\n",
    "    def __init__(self, embedding_dimension):\n",
    "        super(FCAttentionModule, self).__init__()\n",
    "        self.q0 = nn.Linear(embedding_dimension, 1)\n",
    "        self.W = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        r0 = self.q0(x)\n",
    "        r0 = F.softmax(r0, dim=1)\n",
    "        r0 = torch.matmul(torch.transpose(x, 1, 2), r0)\n",
    "        r0 = torch.squeeze(r0, dim=-1)\n",
    "        #r0 = torch.squeeze(r0, dim=1)\n",
    "        \n",
    "        q1 = self.W(r0)\n",
    "        q1 = torch.squeeze(q1, dim=1)\n",
    "        q1 = torch.tanh(q1)\n",
    "\n",
    "        r1 = torch.matmul(x, torch.unsqueeze(q1, dim=-1))\n",
    "        r1 = r1.squeeze(dim=-1)\n",
    "        r1 = F.softmax(r1, dim=1)\n",
    "        r1 = torch.matmul(torch.transpose(x, 1, 2), torch.unsqueeze(r1, dim=-1))\n",
    "        r1 = torch.squeeze(r1, dim=-1)\n",
    "        r1 = torch.squeeze(r1, dim=1)\n",
    "        return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContextGating(nn.Module):\n",
    "    def __init__(self, embedding_dimension):\n",
    "        super(ContextGating, self).__init__()\n",
    "        self.gate = nn.Linear(embedding_dimension, embedding_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        context = self.gate(x)\n",
    "        context = torch.tanh(x)\n",
    "        return context * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.mobilenet = MobileNetV2(n_class=1000)\n",
    "        state_dict = torch.load('mobilenet_v2.pth.tar', map_location='cpu') # add map_location='cpu' if no gpu\n",
    "        self.mobilenet.load_state_dict(state_dict)\n",
    "        self.mobilenet.classifier = Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SplitSEAttentionModel(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SplitBaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SplitBaselineModel, self).__init__()\n",
    "        self.fc_attention_1 = FCAttentionModule(EMBEDDING_DIMENSION)\n",
    "        self.context_gating = ContextGating(EMBEDDING_DIMENSION)\n",
    "        self.classifier = nn.Linear(EMBEDDING_DIMENSION, num_classes)\n",
    "    \n",
    "    def calculate_batch_padding(self, batch):\n",
    "        #print(np.array([x.shape[0] for x in batch]))\n",
    "        return np.max(np.array([x.shape[0] for x in batch]), axis=0)  \n",
    "    \n",
    "    def pad_user(self, user, max_len):\n",
    "        if user.shape[0] < max_len:\n",
    "            zero_pad = torch.zeros(*[max_len - user.shape[0], EMBEDDING_DIMENSION])\n",
    "            tensor_list = [torch.squeeze(user[i, :]) for i in range(user.shape[0])]\n",
    "            for i in range(zero_pad.shape[0]):\n",
    "                tensor_list.append(zero_pad[i, :])\n",
    "            stacked = torch.stack(tensor_list)\n",
    "            #print(stacked.shape)\n",
    "            return stacked\n",
    "        else:\n",
    "            return user\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('~~~~~')\n",
    "        padding_length = self.calculate_batch_padding(x)\n",
    "        padded_shape = [len(x), padding_length, EMBEDDING_DIMENSION]\n",
    "        padded_features = torch.empty(*padded_shape)  # batch x max_length x embedding_dim\n",
    "        for i in range(len(x)):\n",
    "            padded_features[i] = self.pad_user(x[i], padding_length)\n",
    "        #print(padded_features.shape)\n",
    "        aggregated = self.fc_attention_1(padded_features)\n",
    "        #print(aggregated.shape)\n",
    "        gated = self.context_gating(aggregated)\n",
    "        #print(gated.shape)\n",
    "        prediction = self.classifier(gated)\n",
    "        #print(prediction.shape)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EndToEndModel():\n",
    "    def __init__(self, num_classes, finetune=False):\n",
    "        #super(BaselineModel, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        if not finetune:\n",
    "            disable_base_training()\n",
    "        self.fc_attention_1 = FCAttentionModule(EMBEDDING_DIMENSION)\n",
    "        self.context_gating = ContextGating(EMBEDDING_DIMENSION)\n",
    "        self.classifier = nn.Linear(EMBEDDING_DIMENSION, num_classes)\n",
    "    \n",
    "    def disable_base_training(self):\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad  = False\n",
    "            \n",
    "    def calculate_batch_padding(self, batch):\n",
    "        return np.max(batch, axis=1)  \n",
    "    \n",
    "    def pad_user(self, user, max_len):\n",
    "        if user.shape[0] < max_len:\n",
    "            zero_pad = torch.zeros(max_len - user.shape[0], EMBEDDING_DIMENSION)\n",
    "            \n",
    "            return torch.stack(user, zero_pad)\n",
    "        else:\n",
    "            return user\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        padding_length = calculate_batch_padding(x)\n",
    "        padded_shape = [len(x), padding_length, EMBEDDING_DIMENSION]\n",
    "        padded_features = torch.empty(*padded_shape)  # batch x max_length x embedding_dim\n",
    "        for i in range(len(x)):\n",
    "            imgs = x[i]\n",
    "            extracted_features = self.feature_extractor(imgs)\n",
    "            padded_features[i] = pad_user(extracted_features, padding_length)\n",
    "        aggregated = self.fc_attention_1(padded_features)\n",
    "        gated = self.context_gating(aggregated)\n",
    "        prediction = self.classifier(gated)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda \n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIMENSION = 1280\n",
    "NUMBER_OF_EPOCHS = 50\n",
    "INPUT_SIZE = 224\n",
    "SEED = 5046\n",
    "TRANSFORMS = t.Compose([\n",
    "    t.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "    t.ToTensor(),\n",
    "    t.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_number):\n",
    "    SCHEDULER.step()\n",
    "    MODEL.train()\n",
    "    running_loss = 0\n",
    "    for i in range(TRAINSET_BATCHED_LENGTH):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for j in range(BATCH_SIZE):\n",
    "            x, y = TRAINSET[BATCH_SIZE * i + j]\n",
    "            X.append(x.to(DEVICE))\n",
    "            Y.append(y)\n",
    "        Y = torch.stack(Y).to(DEVICE)\n",
    "        \n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        outputs = MODEL.forward(X)\n",
    "        loss = CRITERION(outputs, Y)\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch_number + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_after_epoch(epoch_number, best_loss):\n",
    "    MODEL.eval()\n",
    "    loss = 0\n",
    "    average_precision = 0\n",
    "    average_recall = 0\n",
    "    average_f1 = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(TESTSET_BATCHED_LENGTH):\n",
    "            X = []\n",
    "            Y = []\n",
    "            for j in range(BATCH_SIZE):\n",
    "                x, y = TESTSET[BATCH_SIZE * i + j]\n",
    "                X.append(x.to(DEVICE))\n",
    "                Y.append(y)\n",
    "            Y = torch.stack(Y).to(DEVICE)\n",
    "            \n",
    "            outputs = MODEL.forward(X)\n",
    "            loss += CRITERION(outputs, Y).item()\n",
    "            precision, recall, f1 = calculate_metrics(outputs, Y)\n",
    "            average_precision += precision\n",
    "            average_recall += recall\n",
    "            average_f1 += f1\n",
    "        \n",
    "        average_precision = average_precision / TESTSET_BATCHED_LENGTH\n",
    "        average_recall = average_recall / TESTSET_BATCHED_LENGTH\n",
    "        average_f1 = average_f1 / TESTSET_BATCHED_LENGTH\n",
    "        loss = loss / TESTSET_BATCHED_LENGTH\n",
    "        print('[%d] current loss: %.3f, best loss: %.3f | p=%.3f, r=%.3f, f1=%.3f' %\n",
    "                  (epoch_number + 1, loss, best_loss, average_precision, average_recall, average_f1))\n",
    "        if loss < best_loss:\n",
    "            torch.save(MODEL.state_dict(), CHECKPOINT_PATH)\n",
    "            print('Saving updated Model with loss %.3f' % loss)\n",
    "            return loss\n",
    "        else:\n",
    "            return best_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# load data\n",
    "CHECKPOINT_PATH = './models/FC_Attention_chkpt.pth.tar'\n",
    "DATA_PATH = './data/AmazonFashion6ImgPartitioned.npy'\n",
    "\n",
    "TRAINSET = AmazonFashionDatasetLoader(DATA_PATH, TRANSFORMS, train=True)\n",
    "TRAINSET_BATCHED_LENGTH = int(len(TRAINSET) / BATCH_SIZE)\n",
    "CLASS_WEIGHTS = np.full(TRAINSET.num_categories, 30)\n",
    "\n",
    "TESTSET = AmazonFashionDatasetLoader(DATA_PATH, TRANSFORMS, train=False)\n",
    "TESTSET_BATCHED_LENGTH = int(len(TESTSET) / BATCH_SIZE)\n",
    "\n",
    "# Create Model\n",
    "MODEL = BaselineModel(TRAINSET.num_categories)\n",
    "MODEL.to(DEVICE)\n",
    "\n",
    "# Create Optimizer and Loss\n",
    "OPTIMIZER = optim.Adam(model.parameters(), lr=0.1)\n",
    "SCHEDULER = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "CRITERION = nn.BCEWithLogitsLoss(pos_weight=CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X = 38406, length of Y = 38406\n",
      "length of X = 6778, length of Y = 6778\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = './models/FC_Attention_split_chkpt.pth.tar'\n",
    "NUM_CATEGORIES = 1270\n",
    "\n",
    "DATA_PATH_X = './data/amazon_fashion_x_train.npy'\n",
    "DATA_PATH_Y = './data/amazon_fashion_y_train.npy'\n",
    "\n",
    "TRAINSET = AmazonFashionExtractedDataset(DATA_PATH_X, DATA_PATH_Y, train=True)\n",
    "TRAINSET_BATCHED_LENGTH = int(len(TRAINSET) / BATCH_SIZE)\n",
    "#print(CLASS_WEIGHTS)\n",
    "\n",
    "TESTSET = AmazonFashionExtractedDataset(DATA_PATH_X, DATA_PATH_Y, train=False)\n",
    "TESTSET_BATCHED_LENGTH = int(len(TESTSET) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "MODEL = SplitBaselineModel(NUM_CATEGORIES)\n",
    "MODEL.to(DEVICE)\n",
    "\n",
    "# Create Optimizer and Loss\n",
    "OPTIMIZER = optim.Adam(MODEL.parameters(), lr=0.1)\n",
    "SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER, step_size=10, gamma=0.5)\n",
    "\n",
    "CLASS_WEIGHTS = torch.from_numpy(np.full(NUM_CATEGORIES, 30)).float()\n",
    "\n",
    "CRITERION = nn.BCEWithLogitsLoss(pos_weight=CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.301\n",
      "[1,  1000] loss: 0.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dravk\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\dravk\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\dravk\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\dravk\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] current loss: 1.432, best loss: 100000.000 | p=0.355, r=0.813, f1=0.427\n",
      "Saving updated Model with loss 1.432\n",
      "[2,   500] loss: 0.212\n",
      "[2,  1000] loss: 0.222\n",
      "[2] current loss: 1.600, best loss: 1.432 | p=0.353, r=0.810, f1=0.425\n",
      "[3,   500] loss: 0.199\n"
     ]
    }
   ],
   "source": [
    "best_loss = 100000\n",
    "for ep in range(NUMBER_OF_EPOCHS):\n",
    "    train_one_epoch(ep)\n",
    "    best_loss = eval_after_epoch(ep, best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = AmazonFashionDatasetLoader(DATA_PATH, TRANSFORMS, train=True)\n",
    "#test = AmazonFashionDatasetLoader(DATA_PATH, TRANSFORMS, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FeatureExtractor()\n",
    "model.eval()\n",
    "extract_and_save_features(model, train, len(train), train=True)\n",
    "#extract_and_save_features(model, test, len(test), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[0].shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feat = FeatureExtractorWithMaps()\n",
    "test_feat(torch.zeros(*[1, 3, 224, 224])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
